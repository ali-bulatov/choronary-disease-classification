{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python [conda root]",
      "language": "python",
      "name": "conda-root-py"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.11"
    },
    "colab": {
      "name": "AI_Assignment1_ABulatov.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7KQ-SB9jiLG9",
        "colab_type": "text"
      },
      "source": [
        "### Your name:\n",
        "\n",
        "<pre>Alikhan Bulatov</pre>\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VYCZ08RflzUf",
        "colab_type": "text"
      },
      "source": [
        "## Dataset\n",
        "A retrospective sample of males in a heart-disease high-risk region of the Western Cape, South Africa is given [in this url](https://raw.githubusercontent.com/tofighi/MachineLearning/master/datasets/heart.csv). These data are taken from a larger dataset, described in Rousseauw et al, 1983, South African\n",
        "Medical Journal.\n",
        "\n",
        "Below is a description of the variables:\n",
        "1. **sbp**: systolic blood pressure\n",
        "2. **tobacco**: cumulative tobacco (kg)\n",
        "3. **ldl**: low densiity lipoprotein cholesterol\n",
        "4. **adiposity**\n",
        "5. **famhist**: family history of heart disease (Present, Absent)\n",
        "6. **typea**: type-A behavior\n",
        "7. **obesity**\n",
        "8. **alcohol**: current alcohol consumption\n",
        "9. **age**: age at onset\n",
        "10. **chd**: coronary heart disease\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZDWS-3rNiLHF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "#load csv file\n",
        "df = pd.read_csv('https://raw.githubusercontent.com/tofighi/MachineLearning/master/datasets/heart.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GbwtqqE_tlwv",
        "colab_type": "text"
      },
      "source": [
        "## Classification\n",
        "\n",
        "Our goal is classifying coronary heart disease which has two classes of 0 or 1 using several classifiers based on all features (1 to 9).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uanhB7UtiLHE",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "## EDA\n",
        "\n",
        "\n",
        "* EDA (By using numerical and visual Explaratory Data Analysis answer the following questions:)\n",
        "  * How much is the percentage of each class 0 and 1?\n",
        "  * How many missing values do we have?\n",
        "  * How many categorical variables you have in your features?\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bjIiNrvysXNO",
        "colab_type": "code",
        "outputId": "869f3d10-3638-4d62-8bdb-b5753c11b676",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 359
        }
      },
      "source": [
        "### Your code here\n",
        "df.head(10)             # Explore the dataset\n",
        "df.info()               # Check how many categorical variables are there\n",
        "df.isnull().sum().sum() # Calculate the number of missing values\n",
        "df['chd'].value_counts(normalize=True) * 100  # Calculate the percentage of each class\n",
        "\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 462 entries, 0 to 461\n",
            "Data columns (total 11 columns):\n",
            "row.names    462 non-null int64\n",
            "sbp          462 non-null int64\n",
            "tobacco      462 non-null float64\n",
            "ldl          462 non-null float64\n",
            "adiposity    462 non-null float64\n",
            "famhist      462 non-null object\n",
            "typea        462 non-null int64\n",
            "obesity      462 non-null float64\n",
            "alcohol      462 non-null float64\n",
            "age          462 non-null int64\n",
            "chd          462 non-null int64\n",
            "dtypes: float64(5), int64(5), object(1)\n",
            "memory usage: 39.8+ KB\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    65.367965\n",
              "1    34.632035\n",
              "Name: chd, dtype: float64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ou1vKpQCtqvE",
        "colab_type": "text"
      },
      "source": [
        "<pre>Your answers to questions here </pre>\n",
        "* ~65.4% of people in a given sample have a class 0, and ~34.6% have a class 1 for a coronary disease\n",
        "* We have 0 missing values in the dataset\n",
        "* There is one categorical variable - 'famhist'\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bpJ10hj0sXxt",
        "colab_type": "text"
      },
      "source": [
        "## Preprocessing\n",
        "* Encode the categorical variable\n",
        "* Normalize all the other feature columns\n",
        " \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tXLnAf7JswZ8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# convert categorical columns and drop the row names column\n",
        "\n",
        "df_converted = pd.get_dummies(df,prefix=['famhist'],drop_first=True).drop('row.names', axis=1)\n",
        "\n",
        "# normalzie\n",
        "scaler = StandardScaler()\n",
        "scaler.fit(df_converted)\n",
        "\n",
        "# split data into X (features) and y (target)\n",
        "X = df_converted.iloc[:, df_converted.columns != 'chd']\n",
        "y = df_converted.iloc[:, df_converted.columns == 'chd']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LUuLu_tnsbDC",
        "colab_type": "text"
      },
      "source": [
        "# Grid Search\n",
        "\n",
        "- Make sure you build a full data pipeline as you have learned in the course by using Pipeline() in scikit learn api\n",
        "- Set the random seed to 123 (For splitting or any other random algorithm)\n",
        "- Split data into training (80%) and testing (20%)\n",
        "- Use F1 as the metric for comparing classifiers\n",
        "- Use these classifiers.\n",
        "    - Logistic Regression\n",
        "        - All default parameters\n",
        "    - Random Forest\n",
        "        - tune only: n_estimators: {4, 5, 10, 20, 50} \n",
        "    - KNN Classfier \n",
        "        - tune only: n_neighbors: all odd numbers between 3 and 33\n",
        "    - SVM\n",
        "        - [A practical guide to SVM Classification](http://www.csie.ntu.edu.tw/~cjlin/papers/guide/guide.pdf) particularly in page 5 gives you rule of thumb for tuning hyperparameters:\n",
        "\n",
        "        We recommend a \"grid-search\" on 𝐶 and 𝛾 using cross-validation. Various pairs of (𝐶,𝛾) values are tried and the one with the best cross-validation accuracy is picked. We found that trying exponentially growing sequences of 𝐶 and 𝛾 is a practical method to identify good parameters. For example use the following values for GridSearchCV:\n",
        "\n",
        "$$\n",
        "(\\left.C=2^{-5}, 2^{-3}, \\ldots, 2^{15} ; \\gamma=2^{-15}, 2^{-13}, \\ldots, 2^{3}\n",
        "\\right)\n",
        "$$\n",
        "\n",
        "- Cross-validation with 5-folds\n",
        "- Other hypter paramenters -> Use default\n",
        "\n",
        "\n",
        "### Which classifier with which hyperparameters performs better in the cross validation?\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XbBeb_OHiLHI",
        "colab_type": "code",
        "outputId": "e87613ba-5309-4e56-f22e-94334e2fba5c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 287
        }
      },
      "source": [
        "# to make this notebook's output stable across runs (reproducible results)\n",
        "np.random.seed(123)\n",
        "#Your code here\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "# Create train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2,random_state=123)\n",
        "\n",
        "# Setup the pipeline steps\n",
        "steps_svm = [('scaler', StandardScaler()), ('SVM', SVC())]\n",
        "steps_reg = [('scaler', StandardScaler()), ('logreg', LogisticRegression())]\n",
        "steps_rf = [('scaler', StandardScaler()), ('RF',RandomForestClassifier())]\n",
        "steps_knn = [('scaler', StandardScaler()), ('KNN',KNeighborsClassifier())]\n",
        "  \n",
        "# Create the pipelines\n",
        "pipeline_svm = Pipeline(steps_svm)\n",
        "pipeline_reg = Pipeline(steps_reg)\n",
        "pipeline_rf = Pipeline(steps_rf)\n",
        "pipeline_knn = Pipeline(steps_knn)\n",
        "\n",
        "## SVM\n",
        "# Specify the hyperparameter space for SVM\n",
        "parameters_svm = {'SVM__C':[1, 10, 100,1000], 'SVM__gamma':[0.1, 0.01]}\n",
        "# Instantiate the GridSearchCV object\n",
        "cv_svm = GridSearchCV(pipeline_svm,parameters_svm,cv=5)\n",
        "# Fit to the training set\n",
        "cv_svm.fit(X_train,y_train.values.ravel())\n",
        "# Predict the labels of the test set\n",
        "y_pred_svm = cv_svm.predict(X_test)\n",
        "# Compute and print metrics\n",
        "print(\"SVM\")\n",
        "print(\"F1 Score: {}\".format(f1_score(y_test, y_pred_svm)))\n",
        "print(\"Tuned Model Parameters: {} \\n\".format(cv_svm.best_params_))\n",
        "\n",
        "## Linear Regression\n",
        "# Fit it to the data\n",
        "pipeline_reg.fit(X_train,y_train.values.ravel())\n",
        "y_pred_reg = pipeline_reg.predict(X_test)\n",
        "# Compute and print metrics\n",
        "print(\"Linear Regression\")\n",
        "print(\"F1 Score: {} \\n\".format(f1_score(y_test, y_pred_reg)))\n",
        "\n",
        "## Random Forest\n",
        "# Specify the hyperparameter space for rf\n",
        "parameters_rf = {'RF__n_estimators':[4, 5, 10, 20, 50]}\n",
        "# Instantiate the GridSearchCV object\n",
        "cv_rf = GridSearchCV(pipeline_rf,parameters_rf,cv=5)\n",
        "# Fit to the training set\n",
        "cv_rf.fit(X_train,y_train.values.ravel())\n",
        "# Predict the labels of the test set\n",
        "y_pred_rf = cv_rf.predict(X_test)\n",
        "# Compute and print metrics\n",
        "print(\"Random Forest\")\n",
        "print(\"F1 Score: {}\".format(f1_score(y_test, y_pred_rf)))\n",
        "print(\"Tuned Model Parameters: {} \\n\".format(cv_rf.best_params_))\n",
        "\n",
        "\n",
        "## KNN\n",
        "# Specify the hyperparameter space for KNN\n",
        "parameters_knn = {'KNN__n_neighbors':[3,5,7,9,11,13,15,17,19,21,23,25,27,29,31,33]}\n",
        "# Instantiate the GridSearchCV object\n",
        "cv_knn = GridSearchCV(pipeline_knn,parameters_knn,cv=5)\n",
        "# Fit to the training set\n",
        "cv_knn.fit(X_train,y_train.values.ravel())\n",
        "# Predict the labels of the test set\n",
        "y_pred_knn = cv_knn.predict(X_test)\n",
        "# Compute and print metrics\n",
        "print(\"KNN\")\n",
        "print(\"F1 Score: {}\".format(f1_score(y_test, y_pred_knn)))\n",
        "print(\"Tuned Model Parameters: {} \\n\".format(cv_knn.best_params_))\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "SVM\n",
            "F1 Score: 0.6229508196721312\n",
            "Tuned Model Parameters: {'SVM__C': 1, 'SVM__gamma': 0.1} \n",
            "\n",
            "Linear Regression\n",
            "F1 Score: 0.65625 \n",
            "\n",
            "Random Forest\n",
            "F1 Score: 0.45614035087719296\n",
            "Tuned Model Parameters: {'RF__n_estimators': 20} \n",
            "\n",
            "KNN\n",
            "F1 Score: 0.4912280701754386\n",
            "Tuned Model Parameters: {'KNN__n_neighbors': 19} \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OA6fQmrcyFlf",
        "colab_type": "text"
      },
      "source": [
        "<pre>Linear Regression model with default parameters is the best model </pre>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JNHZ-HP3u--a",
        "colab_type": "text"
      },
      "source": [
        "# Classification Reports \n",
        "\n",
        "\n",
        "Show classification report and confusion matrix **just for the best model you found** during Grid Search. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "56-elS-fu9aB",
        "colab_type": "code",
        "outputId": "8d22c381-a21a-436e-a52f-16f1b9d56d32",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 215
        }
      },
      "source": [
        "#Your code here\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "print(classification_report(y_test, y_pred_reg))\n",
        "print(confusion_matrix(y_test, y_pred_reg),)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.81      0.83      0.82        60\n",
            "           1       0.68      0.64      0.66        33\n",
            "\n",
            "    accuracy                           0.76        93\n",
            "   macro avg       0.74      0.73      0.74        93\n",
            "weighted avg       0.76      0.76      0.76        93\n",
            "\n",
            "[[50 10]\n",
            " [12 21]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IN9ZDxNAiLHL",
        "colab_type": "text"
      },
      "source": [
        "# Conclusions\n",
        "\n",
        "<pre>Summarize and explain your results and choices here </pre>\n",
        "\n",
        "The best model found is the liner regression model because considering our problem which is to predict whether a person has a coronary desease or not it is much more important to predict whether a person has a disease and not whether the person does not have it. <br><br>\n",
        "Looking at the confusion matrix above we can see that we have 12 false negatives and 21 true positives which is the best result among all models, some models were better at predicting true negatives but this is not what our model is supposed to do."
      ]
    }
  ]
}